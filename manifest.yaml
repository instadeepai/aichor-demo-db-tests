# This manifest run the training with:
# - 1 container
# - 1 NVIDIA-A100-SXM4-40GB GPU per container
#
# - mixed_precision bf16
# - batch_size 40
# - epochs 12
# - model: tinyllama
# Execution time: 6m2s.

kind: AIchorManifest
apiVersion: 0.2.2

builder:
  image: image
  context: hugging-face-accelerate # hugging-face-accelerate folder
  dockerfile: ./Dockerfile

spec:
  operator: batchjob # or kuberay
  image: image
  command: "torchrun --nproc_per_node 1 main.py --batch_size 200 --enable_checkpointing true --checkpoint_dir checkpoint_reload --num_epochs 200 --checkpoint_interval 10 --model openai-community/gpt2" # --nproc_per_node=={Number of GPUs}

  restartPolicy:
    backoffLimit: 5

  tensorboard:
    enabled: true

  types:
    Worker:
      count: 1
      resources:
        cpus: 16
        ramRatio: 4
        accelerators: # optional
          gpu:
            count: 2 # note that setting more than 1 MIG device per container is useless since only one will be usable https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#cuda-device-enumeration
            type: gpu
            product: NVIDIA-H100-80GB-HBM3

